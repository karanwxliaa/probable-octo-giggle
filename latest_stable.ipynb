{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'D:\\anaconda\\envs\\TORCHGPU\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "#from .wrapper import CacheClassLabel, MyDataset, AppendName\n",
    "from torch.utils.data import  DataLoader\n",
    "import PIL\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" THIS PART BELONGS TO THE WRAPPER IN DATA\"\"\"\n",
    "import torch.utils.data as data\n",
    "class AppendName(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that also return the name of the dataset/task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, name):\n",
    "        super(AppendName,self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,target = self.dataset[index]\n",
    "        return img, target, self.name\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets,root, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "        self.root = root\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = int(self.targets[index])\n",
    "\n",
    "        if self.transform:\n",
    "            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class MyDataset_BP4D(Dataset):\n",
    "    def __init__(self, data, targets, root, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets  # Assuming targets is a dictionary\n",
    "        self.transform = transform\n",
    "        self.root = root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Assuming self.data is a dictionary with file names as keys\n",
    "        filename = list(self.data.keys())[index]\n",
    "        image = self.data[filename]\n",
    "        target = self.targets[filename]\n",
    "\n",
    "        # Convert NumPy array to PIL Image\n",
    "        image = Image.fromarray(image.astype(np.uint8).transpose(1, 2, 0))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# class DISFADataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.samples = []\n",
    "\n",
    "#         # Load all samples into a list\n",
    "#         for img_folder in sorted(os.listdir(os.path.join(root_dir, 'Images'))):\n",
    "#             images = [Image.open(os.path.join(root_dir, 'Images', img_folder, f'{i:03d}.jpg')).convert('RGB') for i in range(len(os.listdir(os.path.join(root_dir, 'Images', img_folder))))]\n",
    "\n",
    "#             labels = {}\n",
    "#             labels_folder = os.path.join(root_dir, 'Labels', img_folder)\n",
    "#             for au_file in os.listdir(labels_folder):\n",
    "#                 au, _ = os.path.splitext(au_file)\n",
    "#                 au_data = torch.from_numpy(\n",
    "#                     np.loadtxt(os.path.join(labels_folder, au_file), dtype=np.float32, skiprows=2, usecols=1)\n",
    "#                 ).long()\n",
    "#                 labels[au] = au_data\n",
    "\n",
    "#             self.samples.append((images, labels))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images, labels = self.samples[idx]\n",
    "\n",
    "#         if self.transform:\n",
    "#             images = [self.transform(img) for img in images]\n",
    "\n",
    "#         images = torch.stack(images, dim=0)\n",
    "\n",
    "#         return images, labels\n",
    "    \n",
    "\n",
    "# class DISFADataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.samples = []\n",
    "\n",
    "#         # Load all samples into a list\n",
    "#         for img_folder in sorted(os.listdir(os.path.join(root_dir, 'Images'))):\n",
    "#             images = [Image.open(os.path.join(root_dir, 'Images', img_folder, f'{i:03d}.jpg')).convert('RGB') for i in range(len(os.listdir(os.path.join(root_dir, 'Images', img_folder))))]\n",
    "\n",
    "#             labels = {}\n",
    "#             labels_folder = os.path.join(root_dir, 'Labels', img_folder)\n",
    "#             for au_file in os.listdir(labels_folder):\n",
    "#                 au, _ = os.path.splitext(au_file)\n",
    "#                 au_data = torch.from_numpy(\n",
    "#                     np.loadtxt(os.path.join(labels_folder, au_file), dtype=np.float32, skiprows=2, usecols=1)\n",
    "#                 ).long()\n",
    "#                 labels[au] = au_data\n",
    "\n",
    "#             self.samples.append((images, labels))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images, labels = self.samples[idx]\n",
    "\n",
    "#         if self.transform:\n",
    "#             images = [self.transform(img) for img in images]\n",
    "\n",
    "#         # Concatenate images along the batch dimension\n",
    "#         images = torch.stack(images, dim=0)\n",
    "\n",
    "#         # Concatenate labels along the batch dimension\n",
    "#         labels_tensor = torch.stack(list(labels.values()), dim=1)\n",
    "\n",
    "#         return images, labels_tensor\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "#from .wrapper import CacheClassLabel, MyDataset, AppendName\n",
    "from torch.utils.data import  DataLoader\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_image1(filename):\n",
    "    \"\"\"read an image and convert it to numpy array and returns it\"\"\"\n",
    "    img = Image.open(filename)\n",
    "    img.load()\n",
    "    img = img.resize((32,32)) \n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    data = np.asarray(img, dtype=\"int32\")\n",
    "    return data    \n",
    "    \n",
    "\n",
    "\n",
    "def BP4DDataset(root_dir=r'D:\\\\BP4D\\\\',transform=False):\n",
    "\n",
    "\n",
    "    BP4D_IMAGES = root_dir + 'images_crop'\n",
    "    # Images paths and Occurrence labels used to create a pickle file.\n",
    "    #BP4D_OCCURENCE = root_dir + 'aus_bp4d_occurrence.pkl'\n",
    "    BP4D_OCCURENCE = root_dir + \"Filtered_BP4D_OCCURENCE.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Use the same transformation for both training and validation datasets\n",
    "\n",
    "    mypath = BP4D_IMAGES\n",
    "\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "    file = BP4D_OCCURENCE\n",
    "    with open(file, \"rb\") as f:\n",
    "        label = pickle.load(f)\n",
    "\n",
    "    datax = {}\n",
    "\n",
    "    au_list = [\"1\", \"2\", \"4\", \"6\", \"7\", \"10\", \"12\", \"14\", \"15\", \"17\", \"23\", \"24\"]    \n",
    "    for i,f in enumerate(onlyfiles):\n",
    "        datax[f[:-4]] = load_image1(mypath + \"/\" + f)\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Assuming datax is a dictionary with keys and label is a dictionary with corresponding labels\n",
    "\n",
    "    # Extract keys from datax\n",
    "    keys = list(datax.keys())\n",
    "\n",
    "    # Split the keys into training and validation sets\n",
    "    train_keys, val_keys = train_test_split(keys, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Use the split keys to extract corresponding data and labels\n",
    "    train_x = {key: datax[key] for key in train_keys}\n",
    "    train_y = {key: label[key] for key in train_keys}\n",
    "    val_x = {key: datax[key] for key in val_keys}\n",
    "    val_y = {key: label[key] for key in val_keys}\n",
    "\n",
    "    # Now use train_x, train_y, val_x, and val_y in your MyDataset_BP4D constructor\n",
    "    train_datasets = MyDataset_BP4D(train_x, train_y, \"data\", transform)\n",
    "    val_datasets = MyDataset_BP4D(val_x, val_y, \"data\", transform)\n",
    "\n",
    "\n",
    "    return train_datasets, val_datasets\n",
    "        \n",
    "\n",
    "\n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        image_dir = os.path.join(root_dir, 'Image\\\\aligned\\\\aligned')\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = os.path.join(root_dir, 'Annotation/manual/manual')\n",
    "        self.emotion_labels_path = os.path.join(root_dir, 'EmoLabel/list_patition_label.txt')\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        emotion_labels_path = os.path.join(root_dir, 'EmoLabel/list_patition_label.txt')\n",
    "        emotion_labels = pd.read_csv(emotion_labels_path, sep=' ', names=['image_name', 'emotion_label'], header=None)\n",
    "        emotion_labels['image_name'] = emotion_labels['image_name'].str.replace('.jpg', '_aligned.jpg')\n",
    "        self.emotion_labels = emotion_labels\n",
    "\n",
    "        # Loop through the dataset to load images and labels\n",
    "        for idx in range(len(emotion_labels)):\n",
    "            image_name = emotion_labels.iloc[idx, 0]\n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "            if transform:\n",
    "                image = transform(image)\n",
    "\n",
    "            # Load emotion label\n",
    "            emotion_label = emotion_labels.iloc[idx, 1]\n",
    "\n",
    "            self.data.append((image, emotion_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, emotion_label = self.data[idx]\n",
    "\n",
    "        return image, emotion_label\n",
    "    \n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "class ArousalValenceDataset(Dataset):\n",
    "    def __init__(self, data_root, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # Load dataset\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        images_dir = os.path.join(self.data_root, 'images')\n",
    "        annotations_dir = os.path.join(self.data_root, r'3rd ABAW Annotations\\Third ABAW Annotations\\VA_Estimation_Challenge\\Train_Set')\n",
    "\n",
    "        for folder_name in os.listdir(images_dir):\n",
    "            images_folder_path = os.path.join(images_dir, folder_name)\n",
    "            if not os.path.isdir(images_folder_path):\n",
    "                continue\n",
    "\n",
    "            annotation_file = f'{folder_name}.txt'\n",
    "            annotation_path = os.path.join(annotations_dir, annotation_file)\n",
    "            if not os.path.isfile(annotation_path):\n",
    "                continue\n",
    "\n",
    "            # Read arousal and valence values\n",
    "            annotations = pd.read_csv(annotation_path)\n",
    "\n",
    "            for i, row in annotations.iterrows():\n",
    "                image_file = f'{(i+1):05}.jpg'\n",
    "                image_path = os.path.join(images_folder_path, image_file)\n",
    "\n",
    "                if not os.path.isfile(image_path):\n",
    "                    continue\n",
    "\n",
    "                self.samples.append((image_path, row['valence'], row['arousal']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, valence, arousal = self.samples[idx]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor([valence, arousal], dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_image(filename):\n",
    "    \"\"\"read an image and convert it to numpy array and returns it\"\"\"\n",
    "    img = Image.open(filename)\n",
    "    img.load()\n",
    "    #img = img.resize((45,55)) \n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    data = np.asarray(img, dtype=\"int32\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def ConFAR(transform):\n",
    "    # this function reads RAF-DB data and it separates the samples given category. Output as dictionary. (dict[\"1\"] = male samples, dict[\"2\"] = female samples for gender ..)\n",
    "    \n",
    "    transform = transform\n",
    "    \n",
    "    rafdbdata = r'D:\\RAFDB\\RAF-DB\\RAF-DB-20231017T081935Z-001\\RAF-DB\\basic'\n",
    "    disfadata = r\"C:\\Users\\KARAN\\Desktop\\Cambridge\\LibreFace\\data\\DISFA\"\n",
    "    affwild2= r\"C:\\Users\\KARAN\\Desktop\\Cambridge\\DATASETS\\AffWild2\"\n",
    "    \n",
    "    train_datasets = {} # 1: RAF-DB train 2: DISFA Tran and 3: Affwild2 Train\n",
    "    val_datasets = {}\n",
    "\n",
    "    task_output_space = {\"AU\": 12, \"FER\": 7}#, \"AV\": 2}\n",
    "\n",
    "    n_tasks = len(task_output_space)\n",
    "\n",
    "    # for key in task_output_space:\n",
    "    #     if key == 'FER':\n",
    "    #         all_data = RAFDBDataset(transform=train_transform, root_dir=rafdbdata) # update this for FER \n",
    "    #         train_dataset, val_dataset = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "    #         print(\"Loaded RAFDB for FER\")\n",
    "\n",
    "    #     elif key =='AU':\n",
    "    #         all_data = DISFADataset(transform=train_transform, root_dir=disfadata) # update this for AU \n",
    "    #         train_dataset, val_dataset = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "    #         print(\"Loaded DISFA for AU\")\n",
    "\n",
    "    #     train_datasets[key] = AppendName(train_dataset, key)\n",
    "    #     val_datasets[key] = AppendName(val_dataset, key)\n",
    "\n",
    "    for key in task_output_space:\n",
    "        if key == 'FER':\n",
    "            all_data = RAFDBDataset(transform=transform, root_dir=rafdbdata) # update this for FER \n",
    "            train_dataset, val_dataset = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "            print(\"Loaded RAFDB for FER\")\n",
    "\n",
    "        elif key =='AV':\n",
    "            full_data = ArousalValenceDataset(data_root=affwild2, transform=transform)\n",
    "            train_dataset, val_dataset = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "            print(\"Loaded Affwild2 for AV\")\n",
    "\n",
    "        elif key =='AU':\n",
    "            train_dataset, val_dataset = BP4DDataset(transform=transform) # update this for AU \n",
    "            print(\"Loaded BP4D for AU\")\n",
    "\n",
    "        train_datasets[key] = AppendName(train_dataset, key)\n",
    "        val_datasets[key] = AppendName(val_dataset, key)\n",
    "        \n",
    "    \n",
    "\n",
    "    return train_datasets, val_datasets, task_output_space\n",
    "\n",
    "\n",
    "\n",
    "# Create a folder with 3 different sub-folders\n",
    "# 1st folder will be the FER dataset (RAFDB)\n",
    "## Train and Test\n",
    "### 6 clases under each\n",
    "\n",
    "# 2nd folder will be the AU dataset (DISFA)\n",
    "## Train and Test\n",
    "### Images and labels\n",
    "\n",
    "# 3rd folder will be the AV dataset (AffWild2)\n",
    "## Train and Test\n",
    "### Images and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((100, 100)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# a,b,c = ConFAR(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from collections import OrderedDict\n",
    "import agents\n",
    "#import dataloaders.base\n",
    "#from dataloaders.base import ConFAR\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.mkdir('outputs')\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((100, 100)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    #train_dataset_splits, val_dataset_splits, task_output_space = RafDB_perm(args.category, args.train_aug)\n",
    "    train_dataset_splits, val_dataset_splits, task_output_space = ConFAR(transform)\n",
    "\n",
    "    # Print the shape of a sample from the training datasets\n",
    "    for key, dataset in train_dataset_splits.items():\n",
    "        print(f\"Training dataset shape for task {key}: {len(dataset)}\")\n",
    "        sample_x, sample_y, _ = dataset[0]  # Assuming the first sample is representative\n",
    "        print(f\"Training dataset sample shape for task {key} - x: {sample_x.shape}, y: {sample_y.shape}\")\n",
    "\n",
    "    # Print the shape of a sample from the validation datasets\n",
    "    for key, dataset in val_dataset_splits.items():\n",
    "        print(f\"Validation dataset shape for task {key}: {len(dataset)}\")\n",
    "        sample_x, sample_y, _ = dataset[0]  # Assuming the first sample is representative\n",
    "        print(f\"Validation dataset sample shape for task {key} - x: {sample_x.shape}, y: {sample_y.shape}\")\n",
    "\n",
    "\n",
    "    # Decide split ordering\n",
    "    task_names = sorted(list(task_output_space.keys())) #SORT THIS HERE SUXYHDGBFN\n",
    "    #task_names = list(task_output_space.keys()) #SORT THIS HERE SUXYHDGBFN\n",
    "    print('Task order:',task_names)\n",
    "    \n",
    "    agent_config = {'tasks':task_names,'lr': args.lr, 'momentum': args.momentum, 'weight_decay': args.weight_decay,'schedule': args.schedule,\n",
    "                            'model_type':args.model_type, 'model_name': args.model_name, 'model_weights':args.model_weights,\n",
    "                            'out_dim':{'FER':args.force_out_dim_fer,'AU':args.force_out_dim_au} ,\n",
    "                            'optimizer':args.optimizer,\n",
    "                            'print_freq':args.print_freq, 'gpuid': args.gpuid,\n",
    "                            'reg_coef':args.reg_coef}\n",
    "    agent = agents.__dict__[args.agent_type].__dict__[args.agent_name](agent_config)\n",
    "    print(agent.model)\n",
    "    print('# of parameters:',agent.count_parameter())\n",
    "    acc_table = OrderedDict()\n",
    "    f1_table = OrderedDict()\n",
    "    \n",
    "\n",
    "    for i in range(len(task_names)):\n",
    "        \n",
    "        print(len(train_dataset_splits[task_names[i]]))\n",
    "        train_name = task_names[i]\n",
    "        print('======================',train_name,'=======================')\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_splits[train_name],\n",
    "                                                    batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset_splits[train_name],\n",
    "                                                    batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        if args.incremental_class:\n",
    "            agent.add_valid_output_dim(task_output_space[train_name])\n",
    "\n",
    "        # Learn\n",
    "        print(\"cats = \",train_loader, val_loader,train_name,sep=\"  \")\n",
    "        agent.learn_batch(train_loader, val_loader,i)\n",
    "\n",
    "        # Evaluate\n",
    "        acc_table[train_name] = {} #OrderedDict()\n",
    "        f1_table[train_name] = {} #OrderedDict()\n",
    "        for j in range(i+1):\n",
    "            val_name = task_names[j]\n",
    "            print('validation split name:', val_name)\n",
    "            val_data = val_dataset_splits[val_name] if not args.eval_on_train_set else train_dataset_splits[val_name]\n",
    "            val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                                        batch_size=args.batch_size, shuffle=False,\n",
    "                                                        num_workers=0)\n",
    "            acc_table[train_name][val_name], f1_table[train_name][val_name] = agent.validation(val_loader,val_name)\n",
    "            \n",
    "            \"\"\" table's format :\n",
    "            [FER: FER\n",
    "            AU:  FER,  AU]\n",
    "            \"\"\"\n",
    "\n",
    "    return acc_table, task_names, f1_table\n",
    "\n",
    "def get_args(argv):\n",
    "    # This function prepares the variables shared across demo.py\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--gpuid', nargs=\"+\", type=int, default=[0],\n",
    "                        help=\"The list of gpuid, ex:--gpuid 3 1. Negative value means cpu-only\")\n",
    "    parser.add_argument('--model_type', type=str, default='mlp', help=\"The type (mlp|lenet|vgg|resnet) of backbone network\")\n",
    "    parser.add_argument('--model_name', type=str, default='MLP', help=\"The name of actual model for the backbone\")\n",
    "    parser.add_argument('--force_out_dim_fer', type=int, default=7, help=\"Set 0 to let the task decide the required output dimension\")\n",
    "    parser.add_argument('--force_out_dim_au', type=int, default=12, help=\"Set 0 to let the task decide the required output dimension\")\n",
    "    #parser.add_argument('--force_out_dim', type=int, default=7, help=\"Set 0 to let the task decide the required output dimension\")\n",
    "    parser.add_argument('--agent_type', type=str, default='default', help=\"The type (filename) of agent\")\n",
    "    parser.add_argument('--agent_name', type=str, default='NormalNN', help=\"The class name of agent\")\n",
    "    parser.add_argument('--optimizer', type=str, default='SGD', help=\"SGD|Adam|RMSprop|amsgrad|Adadelta|Adagrad|Adamax ...\")\n",
    "    parser.add_argument('--dataroot', type=str, default='data', help=\"The root folder of dataset or downloaded data\")\n",
    "    parser.add_argument('--dataset', type=str, default='MNIST', help=\"MNIST(default)|CIFAR10|CIFAR100\")\n",
    "    parser.add_argument('--n_permutation', type=int, default=0, help=\"Enable permuted tests when >0\")\n",
    "    parser.add_argument('--first_split_size', type=int, default=2)\n",
    "    parser.add_argument('--other_split_size', type=int, default=2)\n",
    "    parser.add_argument('--no_class_remap', dest='no_class_remap', default=False, action='store_true',\n",
    "                        help=\"Avoid the dataset with a subset of classes doing the remapping. Ex: [2,5,6 ...] -> [0,1,2 ...]\")\n",
    "    parser.add_argument('--train_aug', dest='train_aug', default=False, action='store_true',\n",
    "                        help=\"Allow data augmentation during training\")\n",
    "    parser.add_argument('--rand_split', dest='rand_split', default=False, action='store_true',\n",
    "                        help=\"Randomize the classes in splits\")\n",
    "    parser.add_argument('--rand_split_order', dest='rand_split_order', default=False, action='store_true',\n",
    "                        help=\"Randomize the order of splits\")\n",
    "    parser.add_argument('--workers', type=int, default=3, help=\"#Thread for dataloader\")\n",
    "    parser.add_argument('--batch_size', type=int, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=0.01, help=\"Learning rate\")\n",
    "    parser.add_argument('--momentum', type=float, default=0)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0)\n",
    "    parser.add_argument('--schedule', nargs=\"+\", type=int, default=[2],\n",
    "                        help=\"The list of epoch numbers to reduce learning rate by factor of 0.1. Last number is the end epoch\")\n",
    "    parser.add_argument('--print_freq', type=float, default=100, help=\"Print the log at every x iteration\")\n",
    "    parser.add_argument('--model_weights', type=str, default=None,\n",
    "                        help=\"The path to the file for the model weights (*.pth).\")\n",
    "    parser.add_argument('--reg_coef', nargs=\"+\", type=float, default=[0.], help=\"The coefficient for regularization. Larger means less plasilicity. Give a list for hyperparameter search.\")\n",
    "    parser.add_argument('--eval_on_train_set', dest='eval_on_train_set', default=False, action='store_true',\n",
    "                        help=\"Force the evaluation on train set\")\n",
    "    parser.add_argument('--offline_training', dest='offline_training', default=False, action='store_true',\n",
    "                        help=\"Non-incremental learning by make all data available in one batch. For measuring the upperbound performance.\")\n",
    "    parser.add_argument('--repeat', type=int, default=1, help=\"Repeat the experiment N times\")\n",
    "    parser.add_argument('--incremental_class', dest='incremental_class', default=True, action='store_true',\n",
    "                        help=\"The number of output node in the single-headed model increases along with new categories.\")\n",
    "    parser.add_argument('--category', type=str, default='gender', help=\"The Category (gender, race, age)\") \n",
    "    args = parser.parse_args(argv)\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BP4D for AU\n",
      "Loaded RAFDB for FER\n",
      "Training dataset shape for task AU: 932\n",
      "Training dataset sample shape for task AU - x: torch.Size([3, 100, 100]), y: (12,)\n",
      "Training dataset shape for task FER: 1600\n",
      "Training dataset sample shape for task FER - x: torch.Size([3, 100, 100]), y: ()\n",
      "Validation dataset shape for task AU: 233\n",
      "Validation dataset sample shape for task AU - x: torch.Size([3, 100, 100]), y: (12,)\n",
      "Validation dataset shape for task FER: 400\n",
      "Validation dataset sample shape for task FER - x: torch.Size([3, 100, 100]), y: ()\n",
      "Task order: ['AU', 'FER']\n",
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=6912, out_features=192, bias=True)\n",
      "  (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "  (last2): Linear(in_features=96, out_features=48, bias=True)\n",
      "  (last): ModuleDict(\n",
      "    (FER): Linear(in_features=48, out_features=7, bias=True)\n",
      "    (AU): Linear(in_features=48, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "# of parameters: 2012587\n",
      "932\n",
      "====================== AU =======================\n",
      "Incremental class: Old valid output dimension: 0\n",
      "Incremental class: New Valid output dimension: 12\n",
      "cats =   <torch.utils.data.dataloader.DataLoader object at 0x000001836026BDC0>  <torch.utils.data.dataloader.DataLoader object at 0x000001836026BE20>  AU\n",
      "Epoch:0\n",
      "LR: 0.0001\n",
      "Itr\t\tTime\t\t  Data\t\t  Loss\t\tAcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/39]\t1.7383 (1.7383)\t0.0636 (0.0636)\t11.363 (11.363)\t29.17 (29.17)\n",
      "[38/39]\t0.7669 (0.9068)\t0.0188 (0.0212)\t0.000 (1.048)\t30.00 (32.19)\n",
      " * Train Acc 32.189\n",
      "f1 score: 0.26904761904761904\n",
      " * Val Acc 33.047, Total time 3.50\n",
      "validation split name: AU\n",
      "f1 score: 0.26904761904761904\n",
      " * Val Acc 33.047, Total time 3.49\n",
      "1600\n",
      "====================== FER =======================\n",
      "Incremental class: Old valid output dimension: 12\n",
      "Incremental class: New Valid output dimension: 19\n",
      "cats =   <torch.utils.data.dataloader.DataLoader object at 0x000001836026BE20>  <torch.utils.data.dataloader.DataLoader object at 0x000001836026BDC0>  FER\n",
      "Epoch:0\n",
      "LR: 0.0001\n",
      "Itr\t\tTime\t\t  Data\t\t  Loss\t\tAcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/67]\t0.9946 (0.9946)\t0.0025 (0.0025)\t3.014 (3.014)\t0.00 (0.00)\n",
      "[66/67]\t0.5762 (0.8575)\t0.0010 (0.0021)\t1.472 (1.504)\t50.00 (46.06)\n",
      " * Train Acc 46.062\n",
      "f1 score: 0.4157006369426751\n",
      " * Val Acc 57.000, Total time 5.49\n",
      "validation split name: AU\n",
      "f1 score: 0.2843915343915344\n",
      " * Val Acc 28.755, Total time 3.41\n",
      "validation split name: FER\n",
      "f1 score: 0.4157006369426751\n",
      " * Val Acc 57.000, Total time 5.92\n",
      "The Accuracy table is  OrderedDict([('AU', {'AU': 33.047210300429185}), ('FER', {'AU': 28.755364806866954, 'FER': 57.0})])\n",
      "Task AU average acc: 33.047210300429185\n",
      "Task FER average acc: 42.877682403433475\n",
      "===Summary of experiment repeats: 1 / 1 ===\n",
      "The regularization coefficient: 1.0\n",
      "The last avg acc of all repeats: [42.8776824]\n",
      "mean: 42.877682403433475 std: 0.0\n",
      "Loaded BP4D for AU\n",
      "Loaded RAFDB for FER\n",
      "Training dataset shape for task AU: 932\n",
      "Training dataset sample shape for task AU - x: torch.Size([3, 100, 100]), y: (12,)\n",
      "Training dataset shape for task FER: 1600\n",
      "Training dataset sample shape for task FER - x: torch.Size([3, 100, 100]), y: ()\n",
      "Validation dataset shape for task AU: 233\n",
      "Validation dataset sample shape for task AU - x: torch.Size([3, 100, 100]), y: (12,)\n",
      "Validation dataset shape for task FER: 400\n",
      "Validation dataset sample shape for task FER - x: torch.Size([3, 100, 100]), y: ()\n",
      "Task order: ['AU', 'FER']\n",
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=6912, out_features=192, bias=True)\n",
      "  (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "  (last2): Linear(in_features=96, out_features=48, bias=True)\n",
      "  (last): ModuleDict(\n",
      "    (FER): Linear(in_features=48, out_features=7, bias=True)\n",
      "    (AU): Linear(in_features=48, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "# of parameters: 2012587\n",
      "932\n",
      "====================== AU =======================\n",
      "Incremental class: Old valid output dimension: 0\n",
      "Incremental class: New Valid output dimension: 12\n",
      "cats =   <torch.utils.data.dataloader.DataLoader object at 0x00000183601BCCD0>  <torch.utils.data.dataloader.DataLoader object at 0x00000183601BCC40>  AU\n",
      "Epoch:0\n",
      "LR: 0.0001\n",
      "Itr\t\tTime\t\t  Data\t\t  Loss\t\tAcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/39]\t1.0774 (1.0774)\t0.0220 (0.0220)\t10.424 (10.424)\t37.50 (37.50)\n",
      "[38/39]\t0.6383 (0.8883)\t0.0171 (0.0219)\t0.000 (0.655)\t30.00 (32.30)\n",
      " * Train Acc 32.296\n",
      "f1 score: 0.26904761904761904\n",
      " * Val Acc 33.047, Total time 3.11\n",
      "validation split name: AU\n",
      "f1 score: 0.26904761904761904\n",
      " * Val Acc 33.047, Total time 2.73\n",
      "1600\n",
      "====================== FER =======================\n",
      "Incremental class: Old valid output dimension: 12\n",
      "Incremental class: New Valid output dimension: 19\n",
      "cats =   <torch.utils.data.dataloader.DataLoader object at 0x00000183601D20A0>  <torch.utils.data.dataloader.DataLoader object at 0x00000183601BCCD0>  FER\n",
      "Epoch:0\n",
      "LR: 0.0001\n",
      "Itr\t\tTime\t\t  Data\t\t  Loss\t\tAcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/67]\t0.7426 (0.7426)\t0.0021 (0.0021)\t3.807 (3.807)\t0.00 (0.00)\n",
      "[66/67]\t0.5771 (0.8512)\t0.0016 (0.0022)\t1.097 (1.552)\t62.50 (47.69)\n",
      " * Train Acc 47.688\n",
      "f1 score: 0.41686009538950713\n",
      " * Val Acc 57.250, Total time 5.72\n",
      "validation split name: AU\n",
      "f1 score: 0.5307539682539683\n",
      " * Val Acc 37.339, Total time 3.61\n",
      "validation split name: FER\n",
      "f1 score: 0.41686009538950713\n",
      " * Val Acc 57.250, Total time 5.95\n",
      "The Accuracy table is  OrderedDict([('AU', {'AU': 33.047210300429185}), ('FER', {'AU': 37.33905579399141, 'FER': 57.25})])\n",
      "Task AU average acc: 33.047210300429185\n",
      "Task FER average acc: 47.2945278969957\n",
      "===Summary of experiment repeats: 1 / 1 ===\n",
      "The regularization coefficient: 10.0\n",
      "The last avg acc of all repeats: [47.2945279]\n",
      "mean: 47.2945278969957 std: 0.0\n",
      "Loaded BP4D for AU\n",
      "Loaded RAFDB for FER\n",
      "Training dataset shape for task AU: 932\n",
      "Training dataset sample shape for task AU - x: torch.Size([3, 100, 100]), y: (12,)\n",
      "Training dataset shape for task FER: 1600\n",
      "Training dataset sample shape for task FER - x: torch.Size([3, 100, 100]), y: ()\n",
      "Validation dataset shape for task AU: 233\n",
      "Validation dataset sample shape for task AU - x: torch.Size([3, 100, 100]), y: (12,)\n",
      "Validation dataset shape for task FER: 400\n",
      "Validation dataset sample shape for task FER - x: torch.Size([3, 100, 100]), y: ()\n",
      "Task order: ['AU', 'FER']\n",
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=6912, out_features=192, bias=True)\n",
      "  (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
      "  (last2): Linear(in_features=96, out_features=48, bias=True)\n",
      "  (last): ModuleDict(\n",
      "    (FER): Linear(in_features=48, out_features=7, bias=True)\n",
      "    (AU): Linear(in_features=48, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "# of parameters: 2012587\n",
      "932\n",
      "====================== AU =======================\n",
      "Incremental class: Old valid output dimension: 0\n",
      "Incremental class: New Valid output dimension: 12\n",
      "cats =   <torch.utils.data.dataloader.DataLoader object at 0x00000183601D2340>  <torch.utils.data.dataloader.DataLoader object at 0x00000183601D25E0>  AU\n",
      "Epoch:0\n",
      "LR: 0.0001\n",
      "Itr\t\tTime\t\t  Data\t\t  Loss\t\tAcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/39]\t1.3814 (1.3814)\t0.0196 (0.0196)\t9.035 (9.035)\t41.67 (41.67)\n",
      "[38/39]\t2.0239 (1.5005)\t0.0467 (0.0339)\t0.000 (0.638)\t30.00 (33.05)\n",
      " * Train Acc 33.047\n",
      "f1 score: 0.26904761904761904\n",
      " * Val Acc 33.047, Total time 6.61\n",
      "validation split name: AU\n",
      "f1 score: 0.26904761904761904\n",
      " * Val Acc 33.047, Total time 6.87\n",
      "1600\n",
      "====================== FER =======================\n",
      "Incremental class: Old valid output dimension: 12\n",
      "Incremental class: New Valid output dimension: 19\n",
      "cats =   <torch.utils.data.dataloader.DataLoader object at 0x00000183601D2130>  <torch.utils.data.dataloader.DataLoader object at 0x00000183601D2340>  FER\n",
      "Epoch:0\n",
      "LR: 0.0001\n",
      "Itr\t\tTime\t\t  Data\t\t  Loss\t\tAcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\TORCHGPU\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/67]\t2.3436 (2.3436)\t0.0102 (0.0102)\t1.861 (1.861)\t0.00 (0.00)\n",
      "[66/67]\t1.0531 (1.2767)\t0.0030 (0.0029)\t0.874 (1.354)\t68.75 (52.12)\n",
      " * Train Acc 52.125\n",
      "f1 score: 0.41686009538950713\n",
      " * Val Acc 57.250, Total time 9.63\n",
      "validation split name: AU\n",
      "f1 score: 0.23115079365079363\n",
      " * Val Acc 28.755, Total time 5.81\n",
      "validation split name: FER\n",
      "f1 score: 0.41686009538950713\n",
      " * Val Acc 57.250, Total time 9.10\n",
      "The Accuracy table is  OrderedDict([('AU', {'AU': 33.047210300429185}), ('FER', {'AU': 28.755364806866954, 'FER': 57.25})])\n",
      "Task AU average acc: 33.047210300429185\n",
      "Task FER average acc: 43.002682403433475\n",
      "===Summary of experiment repeats: 1 / 1 ===\n",
      "The regularization coefficient: 100.0\n",
      "The last avg acc of all repeats: [43.0026824]\n",
      "mean: 43.002682403433475 std: 0.0\n",
      "reg_coef: 1.0 mean: 42.877682403433475 std: 0.0\n",
      "reg_coef: 10.0 mean: 47.2945278969957 std: 0.0\n",
      "reg_coef: 100.0 mean: 43.002682403433475 std: 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # args = get_args(sys.argv[1:])  UNCOMMENT IN FINAL CODE\n",
    "    args = get_args([\n",
    "    '--gpuid', '0',\n",
    "    '--repeat', '1',\n",
    "    '--optimizer', 'Adam',\n",
    "    '--no_class_remap',\n",
    "    # '--force_out_dim', '7',\n",
    "    '--force_out_dim_fer', '7',\n",
    "    '--force_out_dim_au', '12',\n",
    "    '--schedule', '1',\n",
    "    '--batch_size', '24',\n",
    "    '--model_type', 'custom_cnn',\n",
    "    '--model_name', 'Net',\n",
    "    '--agent_type', 'customization',\n",
    "    #'--agent_name', 'EWC',\n",
    "    '--lr', '0.0001',\n",
    "    '--reg_coef', '1', '10', '100',\n",
    "    '--train_aug'\n",
    "    ])\n",
    "    reg_coef_list = args.reg_coef\n",
    "\n",
    "    avg_final_acc = {}\n",
    "\n",
    "    # The for loops over hyper-paramerters or repeats\n",
    "    for reg_coef in reg_coef_list:\n",
    "        args.reg_coef = reg_coef\n",
    "        avg_final_acc[reg_coef] = np.zeros(args.repeat)\n",
    "        for r in range(args.repeat):\n",
    "\n",
    "            # Run the experiment\n",
    "            acc_table, task_names, f1_table = run(args)\n",
    "            print(\"The Accuracy table is \",acc_table)\n",
    "            \n",
    "            name = args.category if args.train_aug == False else args.category + \"_augmented\"\n",
    "            if not os.path.exists(f'results/{args.category}'):\n",
    "                os.makedirs(f'results/{args.category}')\n",
    "            with open('results/' + args.category + '/'  + name + '.txt', 'a') as f:\n",
    "                f.write(\"\\n\" + str(acc_table) + \"f1: \" +  str(f1_table) + \" repeat:\" + str(r) + \" reg_coef: \" + str(reg_coef)  + \" \" +  str(args.agent_name))\n",
    "            # Calculate average performance across tasks\n",
    "            # Customize this part for a different performance metric\n",
    "\n",
    "            avg_acc_history = [0] * len(task_names)\n",
    "            for i in range(len(task_names)):\n",
    "                train_name = task_names[i]\n",
    "                cls_acc_sum = 0\n",
    "                for j in range(i + 1):\n",
    "                    val_name = task_names[j]\n",
    "                    cls_acc_sum += acc_table[train_name][val_name]\n",
    "                avg_acc_history[i] = cls_acc_sum / (i + 1)                    #err here\n",
    "                print('Task', train_name, 'average acc:', avg_acc_history[i])\n",
    "\n",
    "            # Gather the final avg accuracy\n",
    "            avg_final_acc[reg_coef][r] = avg_acc_history[-1]\n",
    "\n",
    "            # Print the summary so far\n",
    "            print('===Summary of experiment repeats:',r+1,'/',args.repeat,'===')\n",
    "            print('The regularization coefficient:', args.reg_coef)\n",
    "            print('The last avg acc of all repeats:', avg_final_acc[reg_coef])\n",
    "            print('mean:', avg_final_acc[reg_coef].mean(), 'std:', avg_final_acc[reg_coef].std())\n",
    "    for reg_coef,v in avg_final_acc.items():\n",
    "        print('reg_coef:', reg_coef,'mean:', avg_final_acc[reg_coef].mean(), 'std:', avg_final_acc[reg_coef].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCHGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
